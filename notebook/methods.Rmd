---
title: "Methods"
author: "Christopher Tastad"
date: "Last Edit: `r format(Sys.time(), '%Y-%m-%d')`"
bibliography: library.bib
output:
    html_document:
        toc: yes
        toc_float: yes
        theme: paper
---

```{r eval=T,echo=F}
library(knitr)
```

```{r global_options, include=F}
knitr::opts_chunk$set(eval=F)
```

***

# Contacts

```{r eval=T,echo=F}
contacts <- read.csv("~/ocpmi/notebook/csv/contacts.csv")
kable(contacts)
```

***

# Documentation

## Project Directory

```{r}
ocpmi/
├── data # location of any experimental or source dataset content
│   ├── bam
│   ├── gene_ref # gene reference sources for all pipelines
│   └── pt59
├── methods # code, packages, and source files for all pipelines
│   ├── EXCAVATOR2_Package_v1.1.2
│   ├── jason_cnv
│   ├── pbs # job scripts for msi
│   ├── seurat
│   ├── test_script.R
│   ├── tim
│   ├── varscan2
│   └── XCAVATOR
├── notebook # source files for project notebook
│   ├── csv
│   ├── ieee-with-url.csl
│   ├── index.Rmd # log source file
│   ├── library.bib # bibtex master reference citation file
│   ├── methods.Rmd # methods source file
│   ├── references.Rmd
│   ├── results.Rmd
│   ├── _site # notebook site dir - homepage location**
│   ├── _site.yml # yaml file for site knit
│   └── tmp
├── reference # reference documents
│   ├── background
│   ├── dir_res_proposal
│   ├── gene_markers
│   ├── presentations
│   ├── single_cell
│   └── wes
└── results # output for any code or pipeline
    ├── excavator2
    ├── seurat
    └── varscan2

```


## Expt Annotation

For consistency in documenting the wide range of information, I want to have a systematic means of tracking this accross experimentation.

#### General Format

```{r}
* samples: `sample` (control), `sample` (expt) via ; `/path/to/samples`
* job: `00001.mesabi`
* output: `/path/to/output`
* gene_ref: `hg38.fa`, via UCSC; `~/ocpmi/gene_ref`
```

#### exvc2 Format

```{r}
* name: `expName`
* job: `00001.mesabi`
* samples: `sample` (control), `sample` (expt) via ; `/path/to/samples`
* excv2_vars: `50000` (window); `pooling` (analysis mode)
* output: `~/ocpmi/results/excavator2`
* gene_ref: `hg38.fa`, `hg38_sortmerge.bed` via UCSC; `~/ocpmi/gene_ref`
* targets: `GCA_000001405.15_GRCh38.bw` via excv2
```

#### STAR Alignment

```{r}
* samples: `sample` via ocpmi; `~/ocpmi/data/bam/wes1`
* job: `00001.mesabi`
* star_vars: `100` (overhang); `Standard` (SAM attr)
* output: `~/ocpmi/data/bam/wes/star_aligned`
* gene_ref: `hg38.fa`, `hg38.gtf` via UCSC; `~/ocpmi/gene_ref`
```


**Function Highlighting**

In documentation I will use the `text highlight` feature in all cases of a code function name and in most cases of a relevant file name.

***

## Render .R -> .html

It is possible to render markdown output straight from a .R file. This can save the need to create two separate files when running code and producing a formatted output.

https://rmarkdown.rstudio.com/articles_report_from_r_script.html

A markdown report can be compiled from an R script by:

```{r eval=F}
rmarkdown::render("analysis.R") # will generate an html output
rmarkdown::render("analysis.R", "pdf_document")
```

The metadata and output can be modified through special comments.

```{r eval=F}
#' ---
#' title: "Crop Analysis Q3 2013"
#' author: "John Smith"
#' date: "May 3rd, 2014"
#' output: pdf_document
#' ---
```

Additionally, regular markdown formatting can be applied behind this special comment type.

```{r eval=F}
#' A script comment that includes **markdown** formatting.
```

***

## notebook_knit.R

I've renamed my `create_site` file to `notebook_knit` due to a conflict with a different executable. The location of this file is now at `/usr/bin` on my local machine. The updated script is below.

```{r eval=F}
#!/usr/bin/env Rscript

cur_dir <- getwd()
setwd("/home/chris/ocpmi/notebook")

rmarkdown::render("index.Rmd")
rmarkdown::render("results.Rmd")
rmarkdown::render("methods.Rmd")
rmarkdown::render("references.Rmd")
rmarkdown::render_site("index.Rmd")

fl <- Sys.glob("*.html")

ifelse(file.exists(fl), file.remove(fl))

setwd(cur_dir)

```

A key element to make this executable was to include it in an environment directory (i.e. `/usr/bin`) and also make it executable for all users with the command below.

```{r eval=F}
sudo chmod a+x notebook_knit.R
```

***

## Folding elements

Following an example from the Seurat pbmc tutorial, I am incorporating folding page elements in my notebook to reduce length and clutter.

```{r eval=F}
<details>
  <summary>****</summary>
</details>
```

***

***

# Gene Data

## Single Cell

#### Seurat

* Tutorial: [file:///mnt/msi/tasta005/ocpmi/methods/seurat/seurat_tutorial/pbmc3k_tutorial.html](file:///mnt/msi/tasta005/ocpmi/methods/seurat/seurat_tutorial/pbmc3k_tutorial.html)
    - `~/ocpmi/methods/seurat/seurat_tutorial/pbmc3k_tutorial.html`

***

## wes

<details>
  <summary>**from: Andrew re wes location 6/14/19**</summary>
You should now have access to nelsona2, where the somatic DNA exome data is located.  Rebecca and Christy have done the primary work on these samples, so please reach out to them if you have questions about the exome data.  Also, there are directories on there of sequencing from the Illumina TST170 panel, a focused DNAseq panel that has copy number calling built into the stock pipeline (using an algorithm called Craft).

Tim has cross walk files; as you will find out there are multiple different sample identifiers for each surgical specimen.

</details>


<details>
  <summary>**from: Christy re coverage data 7/10/19**</summary>

Just realized I didn't send you the final coverage data. My apologies.
The data is now here: `/home/nelsona2/shared/ovarian_Jan2019/DNA_project/coverage`
There are 3 excel files with coverage as listed below. (There is also a README.txt in the above directory with the same information.) Let me know if you have any questions!
Best,
Christy

OvCA_exome_capture_coverage.xlsx
Coverage by chromosome and exon for the bed file provided by Agilent for the capture (S07604715)


OvCA_exome_exon_flattened_coverage.xlsx
Coverage by chromosome, gene and exon using a bed file created by using the S07604715 capture's target gene names to pull the full exon-level coordinates from UCSC (for RefSeq gene names, Ensembl gene names, miRNAs and other non-coding genes). The purpose of this is to make sure each gene is fully covered by the capture, i.e. to help identify any exons that were left out of the capture bed file because they were too difficult to sequence or didn't perform well, or just by accident. (Based on our clinical experience, this happens more than you might think.) Going to this extra effort to pull the full exon bed files from another source allows us to see when exons have been missed by the capture. For each gene, any overlapping exons were merged so that an individual base within in a gene is only counted once for coverage, and overlapping exons are counted/listed in the spreadsheet as a single "meta-exon".

OvCA_exome_fullreduce_coverage.xlsx
Coverage by chromosome and exon. Same bed file as in OvCA_exome_exon_flattened_coverage.xlsx, however instead of only merging overlapping exons if they occurred w any overlapping exons were merged with each other, regardless of gene. This gives us a bed file with any base in the capture only counted once for coverage
 purposes, even if it is included in multiple exons in the same or different genes.


</details>

<details>
  <summary>**Call w/ Rebecca re wes processing 7/23/19**</summary>

Topic: wes data processing and prep

I was able to finally connect with Rebecca and discuss the nature of the wes data processing that took place earlier this year. It sounds like there has been some distance between when they were actually working on this and now. Nonetheless, she was able to share some general information on their process, which will be very helpful.

```{r}
Description
------------------------------------------------------------
mtg with rebecca
  2019-07-17 what is the reference assembly for the wes data
  2019-07-17 file naming conventions
  2019-07-17 what is a reasonable window size
  2019-07-18 what is the nature of our controls
  2019-07-18 do we have fasta files

1 task
```

**Notes**

* All processing was done in MSI scratch due to the large resource use.
* The data is stored in 2nd tier storage right now as a result.
* BAM files might also be in 2nd tier storage.
* The location of the data is `/nelsona2/shared/ovarian_Jan2019/DNA_project`.
* The directory `/ovarian_Jan2019/singlesamples`, which I had thought was the directory of interest, contains the data for the RNAseq project.
* The sc and wes data were mapped using different reference genomes. sc - GR38 and wes - GR37. **This could be a problem.**
    - We might need to remap the wes data to GR38.
    - I clarified with Tim that **GRCh37 and hg19 are the same**, so I should be able to use hg19 as my reference.
* I need to do a search of `/DNA_project` to see if there are any usable sequence files in there.
* RL is going to move a small dataset from aws so that I can begin a pilot analysis.
* The Freebayes params may be relevant, I should look into those.

**Pipeline**

*Pipeline (in-house) &rarr; Mapping (GR37) &rarr; Preprocessing (`ScanIndel-ovarian.py`) &rarr; Freebayes (variance calling) &rarr; Selective Capture (ovarian specific) &rarr; VCF file output &rarr; Selective Filtering (`fb_VCF_filter.R`) &rarr; ClinVar (Christy)*

```{r}
*Pipeline (in-house) &rarr; Mapping (GR37) &rarr; Preprocessing (`ScanIndel-ovarian.py`) &rarr; Freebayes (variance calling) &rarr; Selective Capture (ovarian specific) &rarr; VCF file output &rarr; Selective Filtering (`fb_VCF_filter.R`) &rarr; ClinVar (Christy)*
```


Project dir: `/nelsona2/shared/ovarian_Jan2019/DNA_project`

* Important scripts are located in `/DNA_project/scanindel_mod`.
* The probe pull down included some non-coding regions.
    - The assumption was that there may be a relevant impact on the coding regions in some cases, so these elements should be recovered and selectively filtered out.
* ClinVar was used to apply annotation to variance that matches existing clinical reference.

</details>

***

#### MSI Processing

**Bulk tumor DNAseq analysis (Nelson_Project_003)**

* Agilent V6 Exome + COSMIC library prep, sequenced on the NovaSeq, 151 bp paired end
    + bmgc dir: `/panfs/roc/groups/3/nelsona2/data_release/umgc/novaseq/181103_A00223_0048_AHGMHLDMXX/Nelson3_Project_003`
    + personal dir: `~/ocpmi/data/bam/wes1`

<details>
  <summary>**Failed RNAseq HiSeq Data - Winterhoff Project 001**</summary>

Update from Andy 2019-08-26

* first attempt failed due to poor quality extractions; ended up using the ClonTech kit to try and salvage the poor quality RNA; also challenged by DNA contamination; three total runs were performed on the HiSeqs to QC and potentially salvage; ultimately all data is a loss
    + `panfs/roc/groups/3/nelsona2/data_release/umgc/hiseq/181019_D00635_0420_ACCVYYANXX/Winterhoff_Project_001`
    + `/panfs/roc/groups/3/nelsona2/data_release/umgc/hiseq/181108_D00635_0422_ACCVWNANXX/Winterhoff_Project_001`
    + `/panfs/roc/groups/3/nelsona2/data_release/umgc/hiseq/181205_D00635_0425_BCD07EANXX/Winterhoff_Project_001_SI`


**Project dir**

* dir: `/nelsona2/shared/ovarian_Jan2019/DNA_project`
* fastq: `/home/nelsona2/data_release/umgc/hiseq/181205_D00635_0425_BCD07EANXX/Winterhoff_Project_001_SI`

**Pipeline**

*Pipeline (in-house) &rarr; Mapping (GR37) &rarr; Preprocessing (`ScanIndel-ovarian.py`) &rarr; Freebayes (variance calling) &rarr; Selective Capture (ovarian specific) &rarr; VCF file output &rarr; Selective Filtering (`fb_VCF_filter.R`) &rarr; ClinVar (Christy)*

```{r}
*Pipeline (in-house) &rarr; Mapping (GR37) &rarr; Preprocessing (`ScanIndel-ovarian.py`) &rarr; Freebayes (variance calling) &rarr; Selective Capture (ovarian specific) &rarr; VCF file output &rarr; Selective Filtering (`fb_VCF_filter.R`) &rarr; ClinVar (Christy)*
```


</details>

<details>
  <summary>**from: Andy re new wes data!!! 2019-08-26**</summary>

```{r}

My understanding of bulk exome tumor method
Andrew Nelson <nels2055@umn.edu>	Mon, Aug 26, 2019 at 9:33 AM
To: Tim Starr <star0044@umn.edu>, Christine Henzler <chenzler@umn.edu>, Jinhua Wang <wangjh@umn.edu>, Rebecca LaRue <larue005@umn.edu>, Joshua Baller <jballer@umn.edu>
Cc: Christopher Tastad <tasta005@umn.edu>, Ying Zhang <zhan2142@umn.edu>
Hi all-

Here is the location in data release of the bulk tumor sequencing data for the OCPMI.  Note that the first set of RNAseq data that came off the HiSeq was a total bust.

The RNAseq projects have gone in under Winterhoff_Project_*, while the DNAseq job went under Nelson_Project_003.

We now have RNAseq bulk data, from good quality RNA, prepped with the Stranded RNAseq library kit, with 25-37M reads, that by the basic QC appears of reasonably good quality.

thanks,
andy




On Mon, Aug 26, 2019 at 8:52 AM Andrew Nelson <nels2055@umn.edu> wrote:

    Correct.  Winterhoff Project 001 is the failed bulk RNAseq experiments performed on the HiSeq.

    I'm putting together some information now.   Will send out soon.

    thanks,
    andy

    On Sat, Aug 24, 2019 at 12:08 PM Tim Starr <star0044@umn.edu> wrote:

        Addendum, I have written that the fastq files for these are located in
        /panfs/roc/data_release/3/umgc/nelsona2/hiseq/181205_D00635_0425_BCD07EANX/Winterhoff_Project_001_SI

        So these are not the bulk exome fastq files?


>         On Aug 24, 2019, at 12:06 PM, Tim Starr <star0044@umn.edu> wrote:
>
>         Hi Andy and Ying
>
>         This helps, but I’m still a little confused.  I also have a umgc report for Winterhoff_project_001, which I thought was the bulk tumor exome, but apparently it is for something else.  Do you know what this one is for?
>
>
>
>
>
>
>         -TIm
>
>
>
>         Tim Starr
>         Asst. Prof. Ob-Gyn & Women’s Health UMN
>         star0044@umn.edu, 612-625-4425
>         Lab website: z.umn.edu/starrlab
>         Candidate Cancer Gene Database
>
>>         On Aug 22, 2019, at 2:53 PM, Andrew Nelson <nels2055@umn.edu> wrote:
>>
>>         Hi Tim-
>>
>>         The library prep kit is correct.  As I was remembering, the bulk tumor exome DNAseq was 150 bp paired on the Nova.  See attached.
>>
>>         best,
>>         andy
>>
>>         On Wed, Aug 21, 2019 at 3:53 PM Timothy Starr <star0044@umn.edu> wrote:
>>
>>             Exome sequencing was done using SureSelectXT Human All Exon V6 + COSMIC platform. https://www.agilent.com/en/promotions/sureselect-human-all-exon-v6 . Sequencing was done on the Illumina HiSeq with 50 bp paired end reads.
>>
>>
>>
>>         --
>>         Andrew Nelson, MD, PhD
>>         Assistant Professor, Department of Lab Medicine and Pathology
>>         Director of Research and Development, M Health Molecular Diagnostics Laboratory
>>         University of Minnesota
>>         Office:  612-273-3328
>>         Pager: 612-899-4097
>>         nels2055@umn.edu
>



    --
    Andrew Nelson, MD, PhD
    Assistant Professor, Department of Lab Medicine and Pathology
    Director of Research and Development, M Health Molecular Diagnostics Laboratory
    University of Minnesota
    Office:  612-273-3328
    Pager: 612-899-4097
    nels2055@umn.edu



--
Andrew Nelson, MD, PhD
Assistant Professor, Department of Lab Medicine and Pathology
Director of Research and Development, M Health Molecular Diagnostics Laboratory
University of Minnesota
Office:  612-273-3328
Pager: 612-899-4097
nels2055@umn.edu

OCPMI_bulk_Seq_file_locations.rtf

```
</details>

##### Sample ID Convention

* PM-1-T17-0146-3_S1_R1_001
* PM-[pathology ID]-[Bionet year]-[Bionet ID]-[Bionet subID]-[Illumina_ID]

***

### BAM Creation

I need to create new bam files from our source wes data since these files were likely lost to scratch. I'll be doing a pilot run with two files that I will then run through each of excv2 and vs2.

<details>
  <summary>**bwa Pilot**</summary>

* src: `/home/nelsona2/data_release/umgc/hiseq/181205_D00635_0425_BCD07EANXX/Winterhoff_Project_001_SI`
* samples: `PM-2-T17-0307-3_S9_R1_001.fastq`, `PM-15-T17-0910-1_S2_R1_001.fastq`; via Andy Nelson group
* output: `~/ocpmi/data/bam/wes/pilot`
* gene_ref: `hg38.fa`, via UCSC; `~/ocpmi/data/gene_ref`

**All bam**

* samples: `*.fastq`; `~/ocpmi/data/bam/wes`
* output: `~ocpmi/data/bam/wes`
* gene_ref: `hg38.fa`, via UCSC; `~/ocpmi/data/gene_ref`

</details>

**bwa Pipeline**

*fastq, gene_ref &rarr; `index` gene ref (bwa bwtsw) &rarr; align &ast;.fastq (bwa mem) + `sort` (samtools) &rarr; output.bam*

```{r}
*fastq, gene_ref &rarr; `index` gene ref (bwa bwtsw) &rarr; align &ast;.fastq (bwa mem) + `sort` (samtools) &rarr; output.bam*
```

**STAR Pipeline**

*gene_ref, gtf &rarr; `genomeGenerate` indexing &rarr; `alignReads` + `sort` &rarr; output.bam &rarr; index `samtools`*

```{r}
*gene_ref, gtf &rarr; `genomeGenerate` indexing &rarr; `alignReads` + `sort` &rarr; output.bam &rarr; index `samtools`*
```

**Directory**

```{r}
ocpmi/data/bam/
├── 1kgp # 1000 genomes project files
└── wes # ocpmi whole exome data
    └── pilot
```

#### STAR Alignment

**ocpmi**

* samples: `*.fastq` via ocpmi; `~/ocpmi/data/bam/wes`
* job: `16806914.mesabim1`
* star_vars: `100` (overhang); `SortedByCoordinate` (SAM attr)
* output: `~/ocpmi/data/bam/wes/star_aligned`
* gene_ref: `hg38.fa`, `hg38.gtf` via UCSC; `~/ocpmi/gene_ref`

**1kgp: NA10847**

* samples: `NA10847_SRR070823.fastq` via ocpmi; `~/ocpmi/data/bam/wes`
* job: `16811628.mesabim1`
* star_vars: `100` (overhang); `SortedByCoordinate` (SAM attr)
* output: `~/ocpmi/data/bam/wes/star_aligned`
* gene_ref: `hg38.fa`, `hg38.gtf` via UCSC; `~/ocpmi/gene_ref`

I was finally able to complete realignment with star. A github tutorial gave me an insight on how to structure my file naming scheme in my for loop since this was the issue. My previous iteration was overwriting each file.

* man: [file:///mnt/msi/tasta005/ocpmi/reference/gene_markers/STARmanual.pdf](file:///mnt/msi/tasta005/ocpmi/reference/gene_markers/STARmanual.pdf)
* https://gist.github.com/ipurusho/f6a6e53e0aa798c44e09c87bdc8b74fd

**bam_star_refindex.pbs**

```{r}
#!/bin/bash -l
#PBS -l walltime=24:00:00,nodes=1:ppn=8,mem=64gb
#PBS -m abe
#PBS -M tasta005@umn.edu
#PBS -N bam_star_refindex
#PBS -o logs/output
#PBS -e logs/error

module load gcc star

cd ~/ocpmi/data/gene_ref/ucsc/hg38

STAR --runThreadN 8 \
    --runMode genomeGenerate \
    --genomeDir ~/ocpmi/data/gene_ref/ucsc/hg38 \
    --genomeFastaFiles ~/ocpmi/data/gene_ref/ucsc/hg38/hg38.fa \
    --sjdbGTFfile ~/ocpmi/data/gene_ref/ucsc/hg38/hg38.gtf \
    --sjdbOverhang 100

```


**bam_star_align.pbs**

```{r}
#!/bin/bash -l
#PBS -l walltime=24:00:00,nodes=1:ppn=16,pmem=3500mb
#PBS -m abe
#PBS -M tasta005@umn.edu
#PBS -N bam_star_align
#PBS -o logs/output
#PBS -e logs/error

module load gcc star

cd ~/ocpmi/data/bam/1kgp/star_aligned

## For looped, compressed
for i in *_R1_001.fastq.gz; do
STAR --runMode alignReads \
    --outSAMtype BAM SortedByCoordinate \
    --readFilesIn $i ${i%_R1_001.fastq.gz}_R2_001.fastq.gz \
    --readFilesCommand zcat \
    --genomeDir ~/ocpmi/data/gene_ref/ucsc/hg38 \
    --runThreadN 16 \
    --outFileNamePrefix ${i%_R1_001.fastq.gz}_
done

### Individual files
#STAR --runMode alignReads \
#    --outSAMtype BAM SortedByCoordinate \
#    --genomeDir ~/ocpmi/data/gene_ref/ucsc/hg38 \
#    --readFilesIn read_1.fastq.gz read_2.fastq.gz \
#    --runThreadN 8 \
#    --outFileNamePrefix NA10847_SRR070823
```

#### bwa Alignmnet

I'll be using bwa for sequence alignment.

* man: http://bio-bwa.sourceforge.net/bwa.shtml

##### Reference Indexing

Command for reference index creation using the bwtsw algorithm:

```{r}
bwa index -a bwtsw reference.fa
```

##### Sorted BAM

Command for direct bwa to bam output using the mem algorithm:

* https://biology.stackexchange.com/questions/59493/how-to-convert-bwa-mem-output-to-bam-format-without-saving-sam-file

```{r}
bwa mem genome.fa reads.fastq | samtools sort -o output.bam -

# multiple threads

bwa mem -t 8 genome.fa reads.fastq | samtools sort -@8 -o output.bam -
```

The following bash script should allow for iteration across all fastq files in a directory.

```{r}
for i in ./*.fastq;
do
    bwa mem -t 8 ~/ocpmi/gene_ref/hg38.fa $i.fastq | samtools sort -@8 -o $i.bam -;
done
```

##### Iterating Sorted BAM w/ parallel

* https://www.biostars.org/p/231491/

```{r}
cd ~/ocpmi/bam/wes
module load bwa samtools parallel

ls *.fastq | parallel -j 2 'bwa mem ~/ocpmi/gene_ref/hg38.fa {} | samtools sort -o {.}.bam'

```

##### BAM Indexing

BAM file indexes can be created using gnu parallel and samtools.

* https://www.biostars.org/p/114921/

```{r}
parallel samtools index ::: *.bam
```

***

***

# MSI

## Directories

#### Project Folder


The root project directory is:

* `/home/starrt2/`

I have a personal directory under the root. At the same level, there is also a shared directory which includes some content that is under active collaboration.

* `/home/starrt2/tasta005`

Processed data can be found in the hiseq folder under data_release. This data is mirrored in the msi file structure. Each file set is named by a unique ID which follows a naming convention given by umgc.

* Look for the leading 6 digit integer as a point of reference for the data_release folder.
* The Starr_Project sub folder is given a number which is considered the MSI and sample ID.

* `/home/starrt2/data_release/umgc/hiseq`

The Analysis directory contains the source data

* illumina-basicQC # quality control data
* cellranger-`*`
    + analysis # ?
    + filtered_gene_bc_matricies # data source that has been filtered by "true cell content"
    + raw_gene_bc_matrices # matrix data that includes unfiltered reads or all read content
    + web_summary.html # interactive cell ranger summary

An example of a complete data source location is below:

* `/home/starrt2/data_release/umgc/hiseq/170627_D00635_0259_ACB8R1ANXX/Starr_Project_037/Analysis/cellranger-Starr_037/filtered_gene_bc_matrices/GRCh38`

#### New Cell Ranger Output

* It sounds like the Cell Ranger processing is a work in progress that is being conducted by Ying. They have 5 pt done, and the data is located at the root directory below.

* `/scratch.global/zhan2142/starr/`

***

## Load/Install R pkgs

* In order to run R in my MSI instance, I need to load the module and install applications.

```{r eval=F}
module load R/3.6.0
R
```

* To install packages in my local home directory, need to define a path.

```{r eval=F}
.libPaths(new='~/R/x86_64-pc-linux-gnu-library/3.6.0')
dir.create('~/R/x86_64-pc-linux-gnu-library/3.6.0', showWarnings = FALSE, recursive = TRUE)
```

* Confirm the path is correct.

```{r eval=F}
.libPaths()
```

* Choose as CRAN mirror.

```{r eval=F}
getCRANmirrors()
chooseCRANmirror(ind=2)
```

* Packages should be able to be installed and loaded normally now.

***

## MSI sshfs

#### Auto mount

I'm using `sshfs` to mount my MSI home folder on my local machine to make it more readily accessible. I already have ssh keys exchanged with MSI, so I can create an executable auto mount bash script, which is called `mnt_msi` located in `/usr/bin` on my local machine.

```{r eval=F}
#!/bin/bash

sshfs tasta005@login.msi.umn.edu: ~/starr_lab/msi_mnt/tasta005/
sshfs tasta005@login.msi.umn.edu:../ ~/starr_lab/msi_mnt/starrt2/
sshfs tasta005@login.msi.umn.edu:/home/nelsona2/ ~/starr_lab/msi_mnt/nelsona2/
sshfs tasta005@login.msi.umn.edu:/panfs/roc/data_release/3/bmgc/ ~/starr_lab/msi_mnt/bmgc/

```

**Unmount**

I've run into the case where a server disconnect causes a hang any time I navigate to this mount point. To reset the mount point use the fusermount command to force the unmount first. I've also created an executable for this, `umnt_msi`, in the same directory.

```{r eval=F}
#!/bin/bash

fusermount -zu ~/starr_lab/msi_mnt/tasta005/
fusermount -zu ~/starr_lab/msi_mnt/starrt2/
fusermount -zu ~/starr_lab/msi_mnt/nelsona2/
fusermount -zu ~/starr_lab/msi_mnt/bmgc/

```

***

## Interactive Computing

It is possible to run an interactive session with the MSI lab space using the `isub` command. Additionally, `qsub` allows for scheduling of jobs in the lab space.

This is useful in the event I want to do development inside MSI rather than just batch jobs. More documentation at the link below:

* https://www.msi.umn.edu/content/interactive-queue-use-isub

The `isub` command can also be given params about compute resources. Here is a decent default setting with *1 node, 8 cores, 16 gigs of ram, and a wall time of 8hrs*:

```{r}
isub -n nodes=1:ppn=4 -m 16GB -w 8:00:00
```

***

## Job Scheduling

All of my pbs scripts are located at

* dir: `~/ocpmi/methods/pbs`
* output: `~/ocpmi/methods/pbs/logs/output`
* errors: `~/ocpmi/methods/pbs/logs/error`

A template pbs script

```{r}
#!/bin/bash -l
#PBS -l walltime=8:00:00,nodes=1:ppn=8,mem=16gb
#PBS -m abe
#PBS -M tasta005@umn.edu
#PBS -N JOBNAME
#PBS -o logs/output
#PBS -e logs/error
cd ~/
module load

```

#### Exit Codes

There is some documentation from Adaptive Computing here

* http://docs.adaptivecomputing.com/torque/4-2-9/help.htm#topics/2-jobs/jobExitStatus.htm?Highlight=exit_status

```{r eval=T,echo=F}
codes <- read.csv("/home/chris/starr_lab/ocpmi/notebook/csv/exit_codes.csv")
kable(codes)
```

#### Bash Loop

This is an example bash loop that runs a function on all file types with a common extension and takes the existing file name to the new output.

```{r}
for i in *.bam; do
    samtools mpileup -f ~/ocpmi/gene_ref/hg38.fa "$i" > ${i%%.*}.mpileup
done
```

***

## MSI Resources

I found a page which I think shows a "real time" update on the current node usage for msi. Hopefully, this will serve to give me a better sense of how to strategize around resource reqs.

* https://umgcdownload.msi.umn.edu/website/pbsnodes/index.html

*** 

## Secondary Storage

**AWS s3**

I setup a second tier storage bucket today in anticipation of capacity issues on our group instance. The bucket location is below.

* `s3://tasta005_2t`

**scratch.global**

I have a directory at scratch.global under `/scratch.global/tasta005`

***

***

# CNV

## Varscan2

Tim wants me to try to use Varscan2 as a method to assess the exome data [@Koboldt2012]. Jinhua had tried this in some fashion previously.

<details>
  <summary>**from: Jinhua 6/14/19**</summary>

If it is for single cell exome DNA copy number call, you may try this tool: SCOPE,   https://www.biorxiv.org/content/10.1101/594267v1.full

For low coverage whole exome CNV,  you may try: EXCAVATOR,  http://sourceforge.net/projects/excavatortool/,  and also VARscan 2.


</details>

#### Project Home

* git: https://github.com/dkoboldt/varscan
* dir: `~/ocpmi/methods/varscan2`
* man:  https://dkoboldt.github.io/varscan/copy-number-calling.html

<details>
  <summary>**vs2 Pilot**</summary>

**vs2 Pilot** (failed)

* samples: `NA10847`, `NA19131` via 1kgp phase3
* gene_ref: `hg38.fa`, via UCSC

I need to build a pileup file with `samtools`. I don't know how long that will take, so I'm letting that run on msi. Currently, I am planning to use the UCSC hg19 fasta reference with BAMs from the 1kgp (`NA10847`, `NA19131`).

**vs2 Pilot** (passed)

* samples: `pm2.bam` (control), `pm15.bam`; `~/ocpmi/data/bam/wes/pilot`
* output: `~/ocpmi/results/varscan2/pilot`
* gene_ref: `hg38.fa`, via UCSC; `~/ocpmi/data/gene_ref`

</details>

**Pipeline**

*&ast;.bam `mpileup` (samtools) &rarr; &ast;.mpileup `copynumber` &rarr; output.copynumber `copyCaller` &rarr; output.copynumber.called &rarr; segmentation (DNAcopy) &rarr; visualization (DNAcopy)*

```{r}
*&ast;.bam `mpileup` (samtools) &rarr; &ast;.mpileup `copynumber` &rarr; output.copynumber `copyCaller` &rarr; output.copynumber.called &rarr; segmentation (DNAcopy) &rarr; visualization (DNAcopy)*
```

**Directory**

```{r}
ocpmi/methods/varscan2/
├── pilot
└── VarScan.v2.4.4.jar

```

***

#### Pileup Creation

The general script format.

```{r}
samtools mpileup -f [reference sequence] [BAM file(s)] > myData.mpileup
```

A bash script to iterate all files.

```{r}
for i in *.cram; do
    samtools mpileup -f ~/ocpmi/gene_ref/hg38.fa "$i" > ${i%%.*}.mpileup
done

```

#### copynumber Command

This is the somatic tool version

```{r}
java -jar VarScan.jar copynumber [normal_pileup] [tumor_pileup] [output] OPTIONS
```

The syntax of the command for copy number calling is most similar to the VarScan somatic tool.

```{r}
java -jar VarScan.jar copynumber normal-tumor.pileup output.basename --mpileup 1
```

Or, if you have independent normal and tumor pileup files:

```{r}
java -jar VarScan.jar copynumber normal.pileup tumor.pileup output.basename
```

#### copyCaller Command

```{r}
java -jar VarScan.jar copyCaller [varScan.copynumber] OPTIONS
```

#### Segmentation

I made an R script that utilizes the `DNAcopy` BioC pacakge for segmentation by the CBS algorithm. Simply, this will take the output of the vs2 `copyCaller` function and build a `cna.object` to perform segmentation and visualization. There are several plotting options with this function. This one currently employs the whole genome view.

* DNAcopy: https://www.bioconductor.org/packages/2.3/bioc/vignettes/DNAcopy/inst/doc/DNAcopy.pdf
* CNA: https://www.rdocumentation.org/packages/DNAcopy/versions/1.46.0/topics/CNA
* plot.DNAcopy: https://www.rdocumentation.org/packages/DNAcopy/versions/1.46.0/topics/plot.DNAcopy

**dnacopy_seg_plot.R**

```{r}
library(DNAcopy)

cn <- read.table("pilot_output.copynumber.called",header=T)
cna.object <- CNA(cbind(cn$normal_depth, cn$tumor_depth), cn$chrom, cn$chr_start, data.type="logratio", sampleid=c("pm2","pm15"))

# Smoothing object
cna.object.smooth <- smooth.CNA(cna.object)

#Segmentation at default parameters
cna.object.smooth.segd <- segment(cna.object.smooth, verbose=1)

#Plot whole studies
pdf('rplot.pdf')
plot(cna.object.smooth.segd, plot.type="w")
dev.off()

```

***

## Excavator2

<details>
  <summary>**from: Jinhua 6/14/19**</summary>

If it is for single cell exome DNA copy number call, you may try this tool: SCOPE,   https://www.biorxiv.org/content/10.1101/594267v1.full

For low coverage whole exome CNV,  you may try: EXCAVATOR,  http://sourceforge.net/projects/excavatortool/,  and also VARscan 2.


</details>

#### Project Home

* dir: `~/ocpmi/methods/EXCAVATOR2_Package_v1.1.2`
* man: [file:///mnt/msi/tasta005/ocpmi/methods/EXCAVATOR2_Package_v1.1.2/docs/EXCAVATOR2 Manual.pdf](file:///mnt/msi/tasta005/ocpmi/methods/EXCAVATOR2_Package_v1.1.2/docs/EXCAVATOR2 Manual.pdf)

<details>
  <summary>**excv2 Pilot**</summary>

* samples: `NA10847` (control); `NA19131` via 1kgp GRCh38; `~/ocpmi/data/bam/1kgp`
* output: `~/ocpmi/results/excavator2/pilot`
* gene_ref: `hg38.fa`, `hg38_sortmerge.bed` via UCSC, `GCA_000001405.15_GRCh38.bw` via excv2
    - .fa: `~/ocpmi/data/gene_ref/ucsc/hg38`, .bw:`~/ocpmi/methods/EXCAVATOR2_Package_v1.1.2/data`
</details>

**Pipeline**

Detailed

*&ast;.bed `sort` (bedtools) &rarr; [SourceTarget.txt (&ast;.bw + gene_ref) + &ast;.bed] `TargetPerla.pl` &rarr; [ExpFilePrep.txt (BAMs) + targets] `EXCAVATORDataPrepare.pl` &rarr; [ExpFileAn.txt (output + controls) + targets `EXCAVATORDataAnalysis.pl` &rarr; cnv output &rarr; `figs_finish_excv2.sh`*

Condensed

*&ast;.bed `sort` (bedtools) &rarr; SourceTarget.txt `TargetPerla.pl` &rarr; ExpFilePrep.txt `EXCAVATORDataPrepare.pl` &rarr; ExpFileAn.txt `EXCAVATORDataAnalysis.pl` &rarr; cnv output &rarr; `figs_finish_excv2.sh`*

```{r}
*&ast;.bed `sort` (bedtools) &rarr; SourceTarget.txt `TargetPerla.pl` &rarr; ExpFilePrep.txt `EXCAVATORDataPrepare.pl` &rarr; ExpFileAn.txt `EXCAVATORDataAnalysis.pl` &rarr; cnv output &rarr; `figs_finish_excv2.sh`*
```

Modified

*&ast;.bed `sort` (bedtools) &rarr; `full_send.sh` &rarr; cnv output &rarr; `figs_finish_excv2.sh`*

```{r}
*&ast;.bed `sort` (bedtools) &rarr; `full_send.sh` &rarr; cnv output &rarr; `figs_finish_excv2.sh`*
```

**Directory**

```{r}
ocpmi/methods/EXCAVATOR2_Package_v1.1.2/
├── data
│   ├── centromere
│   ├── GCA_000001405.15_GRCh38.bw # hg38 bigwig file
│   ├── prepare # output of EXCAVATORDataPrepare.pl
│   ├── support
│   ├── targets # output of TargetPerla.pl
│   └── ucsc.hg19.bw
├── docs
│   └── EXCAVATOR2 Manual.pdf
├── EXCAVATORDataAnalysis.pl # excv2 function step 3.
├── EXCAVATORDataPrepare.pl # excv2 function step 2.
├── ExperimentalFileAnalysis.50000.txt # excv2 sourcefile step 3.
├── ExperimentalFilePrepare.50000.txt # excv2 sourcefile step 2.
├── lib
│   ├── bash
│   ├── F77
│   ├── OtherLibrary
│   ├── perl
│   └── R
├── ParameterFile.txt
├── SourceTarget_hg38.txt # excv2 sourcefile step 1.
├── SourceTarget.txt_
└── TargetPerla.pl # excv2 function step 1.

```

***

### Quick Start

The Excv2 workflow analysis is made of three steps that can be invoked by means of three Perl scripts:

0. bed file sort

```{r}
sort -k1,1 -k2,2n *.bed | bedtools merge > file_sorted.bed
```

1. `TargetPerla.pl`

```{r}
perl TargetPerla.pl SourceTarget.txt myTarget.bed MyTarget_w50000 50000 hg38
```

2. `EXCAVATORDataPrepare.pl`

```{r}
perl EXCAVATORDataPrepare.pl ExperimentalFilePrepare.50000.txt --processors 8 --target MyTarget_w50000 --assembly hg38
```

3. `EXCAVATORDataAnalysis.pl`

```{r}
perl EXCAVATORDataAnalysis.pl ExperimentalFileAnalysis.50000.txt --processors 8 --target MyTarget_w50K --assembly hg38 --output /.../path/to/output --mode ...
```

***

#### figs_finish_excv2.sh

I wanted to automate the final processing of the figure output for the excv2 pipeline, so I wrote a bash script to do that. This creates two items:

* A `combined.pdf` which is a string of all the independent figures.
* An image of a 5x5 matrix of each figure on one page.

```{r}
#!/bin/sh

dir=$PWD
cd $dir

pdfunite Plot* combined.pdf
pdfnup Plot* --nup 5x5 --outfile tmp.pdf
pdftoppm tmp.pdf fig_matrix.png -png
rm -rf tmp.pdf
```

***

### full_send.sh

I created a bash script that executes the entirety of the excv2 pipeline. This script has a helper file that is used to define the variables for the pipline in one centralized place. Generally, this script should be portable and could be used by others with limited to no tweaking necessary.

```{r}
#!/bin/bash
# This script is meant to prep the configuration and execution of the Excavator2 pipeline. It auto generates two text files that serve as configs for the pipeline scripts. Proper use of this script is paired with the full_send_vars.txt variable file to indicate pointers and variable names. Both should be placed in the Excavator2 working directory with this script being executed from there.

source full_send_vars.txt
cd $HOME$bamDir

## Overwrite existing config files and updating some vars
rm -f ExperimentalFile*
expName="${expName}_${window}_`date +"%Y-%m-%d_%H%M"`"
target="${expName}_target"
# runTp1=1 # step 1 TargetPerla.pl
# runDp2=1 # step 2 EXCAVATORDataPrepare.pl
# runDa3=1 # step 3 EXCAVATORDataAnalysis.pl

## Loop to iterate through each bam to create string for FilePrepare and FileAnalysis configs
count=1
for i in *.bam; do
    echo "$PWD/$i" "data/prepare/$expName/${i%.*}" "${i%.*}" >> ExperimentalFilePrepare.$expName.txt
    echo "T$count" "data/prepare/$expName/${i%.*}" "${i%.*}" >> ExperimentalFileAnalysis.$expName.txt
    mkdir -p $HOME$excv2Dir/data/prepare/$expName/${i%.*}
    count=$[count+1]
done

## Function to remove leading path from sshfs local mount path and create control
sed -i -e 's/\/home\/chris/~/g' ./ExperimentalFilePrepare.$expName.txt
sed -i -e '$s/T[[:digit:]]*/C1/g' ./ExperimentalFileAnalysis.$expName.txt

## Relocate configs to Excavator working dir
mv ExperimentalFile* $HOME$excv2Dir
mkdir -p $HOME$resDir/$expName
mkdir -p $HOME$excv2Dir/config_files/$expName

## Generate SourceTarget.txt and execute step 1 TargetPerla.pl

# if [[ $runTp1 = 1 ]]; then
#     echo "$HOME$bigWig" "$HOME$genRef" > SourceTarget.$expName.txt
#     perl TargetPerla.pl SourceTarget.$expName.txt $HOME$bedFile $target $window $refAssm;
# else
#     echo "Skipping step 1 TargetPerla.pl"
# fi

cd $HOME$excv2Dir
cp ParameterFile.txt ParameterFile.$expName.txt
cp full_send_vars.txt full_send_vars.$expName.txt

## Generate SourceTarget file and run step 1
echo "$HOME$bigWig" "$HOME$genRef" > SourceTarget.$expName.txt
perl TargetPerla.pl SourceTarget.$expName.txt $HOME$bedFile $target $window $refAssm;

## Executing step 2 EXCAVATORDataPrepare.pl
perl EXCAVATORDataPrepare.pl ExperimentalFilePrepare.$expName.txt --processors $numProc --target $target --assembly $refAssm

## Executing step 3 EXCAVATORDataAnalysis.pl
perl EXCAVATORDataAnalysis.pl ExperimentalFileAnalysis.$expName.txt --processors $numProc --target $target --assembly $refAssm --output $HOME$resDir/$expName --mode $anMode

## Copy config files for record
mv *.$expName.* $HOME$excv2Dir/config_files/$expName

## Cleanup bad file naming convention
cd $HOME$resDir/$expName/Plots
for i in */ ; do
    cd $i
    for j in {1..9}; do
        mv PlotResults_chr$j.pdf PlotResults_chr0$j.pdf
    done
    cd ../
done

```

#### full_send_vars.sh

This is the helper file for the full_send.sh script. All variables need to be filled while many can remain the same accross several experiements.

* Selecting a different experiment name for each series of analysis is critical to avoid overwriting previous output.

```{r}
################################################################################
# This is the config file for full_send.sh
#
# Use of this config file should be paired with full_send.sh.
# Both should be placed in the same directory and executed there.
#
# After setting all variables, the function can be executed with the following
# command in the Excavator2 working directory
#
# ./full_send.sh
#
# This function will overwrite any existing config or target files, so care
# should be taken to use a unique experiment name variable for every case.
################################################################################


################################################################################
# Analysis variables
################################################################################
# Experiment name
expName="ocpmi_v_1kgp_full_50k1"
# Window size
window="50000"
# Number of processors for parallelization
numProc="8"
# Step 3 EXCAVATORDataAnalysis.pl sample analysis mode [pooling, paired]
anMode="pooling"

################################################################################
# Gene reference target files
################################################################################
# Reference assembly to use [hg19,hg38]
refAssm="hg38"
# Location of genome reference fasta
genRef="/ocpmi/data/gene_ref/ucsc/hg38/hg38.fa"
# Location of bed file
bedFile="/ocpmi/data/gene_ref/ucsc/hg38/hg38_sortmerge.bed"
# Location of BigWig file
bigWig="/ocpmi/methods/EXCAVATOR2_Package_v1.1.2/data/GCA_000001405.15_GRCh38.bw"

################################################################################
# Location of working and data directories
################################################################################
# Location of the Excavator working directory
excv2Dir="/ocpmi/methods/EXCAVATOR2_Package_v1.1.2"
# Location of the bam files
bamDir="/ocpmi/data/bam/wes"
# Location of results directory [format will be /path/to/dir/$expName]
resDir="/ocpmi/results/excavator2"
```

### figs_finish_excv2.sh

This is a continuation of the bash script I started yesterday. I've expanded it quite a bit to make it effectively pump and dump for the markdown output needed to go to the `results` page. It needs to be run from the `Plots` results directory.

```{r}
#!/bin/bash

# Establish path for function working dir
dir=$PWD

# Get experiment name from parent dir
cd ..
expName1=$(pwd | awk -F'/' '{print $NF}')
cd $dir

# Source variable file from experiment
source /home/chris/ocpmi/methods/EXCAVATOR2_Package_v1.1.2/config_files/$expName1/full_send_vars.$expName1.txt

# Text for header in figures markdown file
header=$(cat <<EOF
### $expName1 {.tabset .tabfade}

#### Discussion

* name: \`$expName1\`
* samples: \`SAMPLE\` (control), \`SAMPLE\` (expt) via ; \`$bamDir\`
* excv2_vars: \`$window\` (window); \`$anMode\` (analysis mode)
* output: \`$resDir/$expName1\`
* gene_ref: \`hg38.fa\`, \`hg38_sortmerge.bed\` via UCSC; \`~/ocpmi/gene_ref\`
* targets: \`GCA_000001405.15_GRCh38.bw\` via excv2

#### Figures {.tabset .tabfade}

##### Collated

EOF
)

echo "${header}" > figs.md
> collated.md

# Loop to created collated figures and output formatted markdown
for i in */; do
    if [ "$(ls -A $i)" ]; then
        cd $i
        # Create figs
        pdfunite Plot* combined.pdf
        pdfnup Plot* --nup 5x5 --outfile tmp.pdf
        pdftoppm tmp.pdf fig_matrix -png
        rm -rf tmp.pdf
        # Assign variable names for figs
        sampleName=${PWD##*/}
        figMatrix=$(find $PWD -name "fig_matrix*" | grep $sampleName)
        combinedPdf=$(find $PWD -name "combined.pdf" | grep $sampleName)
        # Formatted output to individual sample entry
        sampleMd=$(cat <<EOF

##### ${sampleName}

**Figure Matrix**

![]($figMatrix)

***

**Individual Output**

\`\`\`{r, out.width = "100%", out.height = "600px", echo = F, eval=T}
knitr::include_graphics("${combinedPdf}")
\`\`\`
EOF
)
        # Formatted output to collated entry for all samples
        collatedMd=$(cat << EOF

![${sampleName}]($figMatrix)
EOF
)
        # Pass to markdown files
        echo "${sampleMd}" >> ../figs.md
        echo "${collatedMd}" >> ../collated.md
        cd ../
    else
        cd $i
        sampleName=${PWD##*/}
        # Formatted output for no output
        noOutput=$(cat <<EOF

##### ${sampleName}

**No output**
EOF
)
        # Pass to markdown files
        echo "${noOutput}" >> ../figs.md
        echo "
        ${sampleName} No Output" >> ../collated.md
        cd ../
    fi
done

# Insert contents of collated.md at top of sample entries
cd $dir
sed -i -e '/##### Collated/r collated.md' figs.md
```

***

### Installation

Decompress the Excavator package, then move to `.../excavator/lib/F77` folder and compile the fortran files `F4R.f` and `FastJointSLMLibrary.f` with R compiler:

```{r eval=F}
R CMD SHLIB F4R.f
R CMD SHLIB FastJointSLMLibraryI.f
```

This will create the .o and .so fortran libraries.

#### Important

* Coherence of chromosome annotation among all files is necessary. (chrN vs N)
* The target bed file must be sorted by chromosome number and region coordinate. No overlapping regions must be present in the target file.


#### TargetPerla.pl

`TargetPerla.pl` is the module for target initialization. It creates a pseudo-target file with coordinates from the user-specified target input file (.bed) and calculates GC content and mapability values.

Setting the label name as “MyWindowLabelName”, the `TargetPerla.pl` module will create a folder (if you are using the hg19 assembly) /.../Excavator/data/targets/hg19/MyLabelName containing all files required for Excv2 analysis.


`TargetPerla.pl` requires 5 arguments:

1. Path to a source file (`SourceTarget.txt`)
2. Path to a target input (bed file)
3. A "label"
4. Window size (e.g. 50000)
5. the desired ref assembly (GRc37/GRc38)

An example cmd is:

```{r}
perl TargetPerla.pl SourceTarget.txt myTarget.bed MyTarget_w50000 50000 hg19
```

**The `TargetPerla.pl` working command is:**

```{r}
perl TargetPerla.pl SourceTarget.txt /home/chris/starr_lab/ocpmi/methods/EXCAVATOR2_Package_v1.1.2/data/gene_ref/ucsc/hg19.bed myLabel 50000 hg19
```

The default source file is SourceTarget.txt that is placed in the main program folder. SourceTarget.txt is a space delimited file that contains the absolute paths to

1. bigWig file (.bw) for mappability
2. reference sequence in .fasta

An example `SourceTarget.txt`

```{r}
/.../path/to/file.bw /.../path/to/file.fasta
```

**The working `SourceTarget.txt`**

```{r}
/home/chris/starr_lab/ocpmi/methods/EXCAVATOR2_Package_v1.1.2/data/gene_ref/ucsc/wgEncodeUwRepliSeqBg02esG1bPctSignalRep1.bigWig /home/chris/starr_lab/ocpmi/methods/EXCAVATOR2_Package_v1.1.2/data/gene_ref/ucsc/hg19.fa

```

#### EXCAVATORDataPrepare.pl

`EXCAVATORDataPrepare.pl` is a Perl script managing RC calculations, data normalization and data anal-ysis on multiple .bam files. It requires one argument and 4 command-line options to run properly.

1. a sample input text file
2. processor count
3. the target
4. the ref assembly

An example cmd:

```{r}
perl EXCAVATORDataPrepare.pl ExperimentalFilePrepare.50000.txt --processors 4 --target MyTarget_w50000 --assembly hg19
```

The `ExperimentalFilePrepare.window.txt` file requires 3 items:

1. Direct path to BAM file
2. Path to the output directory
3. Sample name (for output prefix)

An example `ExperimentalFilePrepare.window.txt`

```{r}
/.../path/to/file.bam /.../path/to/output sample_name
```


**The working `ExperimentalFilePrepare.50000.txt`**

```{r}
/home/chris/starr_lab/ocpmi/methods/EXCAVATOR2_Package_v1.1.2/data/gene_ref/ucsc/wgEncodeUwRepliSeqBg02esG1bAlnRep1.bam /home/chris/starr_lab/ocpmi/methods/EXCAVATOR2_Package_v1.1.2/data/output/pilot/g1b g1b
/home/chris/starr_lab/ocpmi/methods/EXCAVATOR2_Package_v1.1.2/data/gene_ref/ucsc/wgEncodeUwRepliSeqBg02esG2AlnRep1.bam /home/chris/starr_lab/ocpmi/methods/EXCAVATOR2_Package_v1.1.2/data/output/pilot/g2 g2

```

#### EXCAVATORDataAnalysis.pl

`EXCAVATORDataAnalysis.pl` is a multi-ghreaded perl script performing segmentation of the WMRC by means of the Shifting Level Model algorithm along with the FastCall algorithm to classify each region as one of 5 possible discrete states:

1. 2-copy deletion
2. 1-copy deletion
3. normal
4. 1-copy duplication
5. N-copy amplification

`EXCAVATORDataAnalysis.pl` requires 6 arguments:

1. a sample input text file
2. processor count
3. the target
4. the ref assembly
5. results output dir
6. mode (paired, pooled)

An example cmd:

```{r}
perl EXCAVATORDataAnalysis.pl ExperimentalFileAnalysis.50000.txt --processors 4 --target MyTarget_w50K --assembly hg19 --output /.../path/to/output --mode ...
```

The `ExperimentalFileAnalysis.window.txt` file requires 3 items, the 2nd and 3rd of which will be the same as the `ExperimentalFilePrepare.window.txt` file.

1. Analysis type option (T or C)
2. `ExperimentalFilePrepare.window.txt` output dir
3. `ExperimentalFilePrepare.window.txt` sample prefix

An example `ExperimentalFileAnalysis.window.txt`

```{r}
T1 /.../path/to/output sample_name
T2 /.../path/to/output sample_name
C1 /.../path/to/output sample_name
```

***

***

## Contra

Juan had tried using contra in the past which did not work.


<details>
  <summary>**from: Juan 6/13/2019**</summary>
Sure thing. In a nutshell back in 2013 I ran Contra with Tumor/Normal Samples and it worked, I got results... fast forward 6 years and the test files are not even working for me.


First, I attempted via Python 3, 2.7 and the original 2.6 version using the test set in my local computer. Then, I moved into the "Matrix" or in this case Mesabi.

With Help from Master Jedi Christy (Thank you Christy!) I created a conda environment for Contra via:

```{r eval=F}
conda create -n contra python=2.6
source activate /home/umgc-staff/abrah023/bin/miniconda3/envs/contra
```

change directory to folder containing Contra script (tried both Version 2.0.8 and 1.0.3).

```{r eval=F}

python contra.py --target ../0247401_D_BED_20090724_hg19_MERGED.bed --test ../P0667N_GATKrealigned_duplicates_marked.bam --control ../P0667T_GATKrealigned_duplicates_marked.bam --fasta human_g1k_v37.fasta --outfolder ../output2/
```

That was using the files that came with Contra and got an error:

```{r eval=F}


(contra) abrah023@ln0003 [~/AndyTemp/CONTRA.v2.0.8] % python contra.py --target ../0247401_D_BED_20090724_hg19_MERGED.bed --test ../P0667N_GATKrealigned_duplicates_marked.bam --control ../P0667T_GATKrealigned_duplicates_marked.bam --fasta human_g1k_v37.fasta --o ../outputr/

target		: ../0247401_D_BED_20090724_hg19_MERGED.bed

test		: ../P0667N_GATKrealigned_duplicates_marked.bam

control		: ../P0667T_GATKrealigned_duplicates_marked.bam

outfolder	: ../outputr/

numBin		: [20]

minreaddepth	: 10

minNBases	: 10

sam		: False

pval		: 0.05

sampleName	: No-SampleName

nomultimapped	: False

plot		: False

bedInput		: False

minExon		: 2000

largeDeletion	: False

removeDups	: False

Creating Output Folder :  Done.

Traceback (most recent call last):

  File "contra.py", line 625, in <module>

    main()

  File "contra.py", line 570, in main

    get_genome(params.TEST, genomeFile)

  File "/panfs/roc/groups/8/umgc-staff/abrah023/AndyTemp/CONTRA.v2.0.8/scripts/get_chr_length.py", line 31, in get_genome

    raw_header = subprocess.Popen(args, stdout = subprocess.PIPE).communicate()[0]

  File "/home/umgc-staff/abrah023/bin/miniconda3/envs/contra/lib/python2.6/subprocess.py", line 623, in __init__

    errread, errwrite)

  File "/home/umgc-staff/abrah023/bin/miniconda3/envs/contra/lib/python2.6/subprocess.py", line 1141, in _execute_child

    raise child_exception

OSError: [Errno 2] No such file or directory

```


</details>

***

## Jason's Work


Jason sent me his working directory for the early phase of the CNV project. It is located at

* `~/ocpmi/methods/jason_cnv`

<details>

  <summary>**from: Jason 6/15/19**</summary>
I have previously explored large scale CNV using the single cell data. I’ve attached my directory of scripts and figures. Please go ahead and use whatever you can from them. The coding might be a little messy… not exactly plug-n-chug, but you should be able to get some use out of it.

As you may know, the single cell data isn’t really fit for gene-level CNV analysis because only 10-15% of the gene space is covered. So my approach was to look at the average expression level of all genes measured in each chromosome arm, for each cell. I scaled these average values across all the cells (vertically in the figures) that we had annotated as non-epithelial cells. The heatmaps show chromosome arms across the X axis and individual cells across the Y axis. So, rather than looking for CNV in a gene, I’m looking for entire chromosome arm amplifications & deletions.

One useful file will be “ENSEMBLgenes.csv” which indicates the chromosome number and location of each gene. Also, in the CNV.Rmd notebook I have hardcoded the centromere position of each chromosome. Together these will allow you to explore average gene expression by chromosome arm.

Please note that if you are using the most recent patient data from Tim, there will be more cells in each patient because we re-ran the Cell Ranger pipeline since I did this analysis. We have also updated the cell type annotation for each cell since I’ve done this.

The vertical colored bar on the left of the heatmaps show the cell type. They are colored according to “Picture1.png”. On the right side of the heatmaps in small font you will see the cluster number and the annotated cell type according to our outdated Seurat pipeline.

If you have any question or would like to discuss in more detail, I’ll be happy to connect with you sometime this week to have a look together. I live in Colorado so we would use Skype or Google Hangouts if you’d like to do this.

</details>

***

## Reference

#### CNV w/ scRNA @Tirosh2016

This paper is the only other case Tim is aware of where a group was able to effectively show CNV in single cell expression data.

* This group took human oligodendrogliomas and isolated ~4300 cells for sequencing.
* They constructed a developmental program from genome-wide expression signatures.
* The analysis produced the conclusion that these cells display 2 distinct developmental programs.
* Subclonal point mutation and insitu hybridization was also done as verification of these results.

##### CNV Method

* Their CNV approach was to define a window along a chromosomal mapping. By doing this, they effectively created "bins" of expression level to define relative changes in expression.
* They identified two cell types that aligned with a genetic character (high values at 1p and 19q) which they defined as baseline expression.
* This was used to define the baseline for microglia and oligdendrocytes.
* PCA comprised the main statistical approach.

***

#### Comparing CNV tools @Zare2017

This paper explored a comparative analysis between current CNV analysis tools that operate in the context of whole exome sequencing which in wide use in the field. The group narrowed the landscape down to just 5 tools that meet a few criteria.

* There are 3 approaches to identifying CNVs
    + read count (read depth)
    + paired-end
    + assembly
* All tools the group evaluated used the read depth method since the pair-end and assembly methods are rendered ineffective with low read coverage like is seen in exome data. All CNV detection tools for WES are based on this method as a result.
* Ambiguities in alignment can create bias in the RD approach.
* The hybridization process and an uneven distribution of reads in exonic regions leads to bias.
* There are reads in the WES data that will be overlapping noise which would otherwise be delineated in the scRNA data. I will have to account for this.
* As of 2016, there are 15 seq-based CNV tools.
* B allele frequencies (BAF) is used to address tumor complexity in some of these tools.
* Group used both read and synthesized data.
* Look at table 1 for feature breakdown
* Look at table 4 for performance summary

##### ADTEx

* Newest of the tools they evaluated
* Has ability to leverage BAF
* Uses Discrete Wavelet Transform as a preprocessing step to reduce noise.
* Two Hidden Markov Models are used in detection.
* CBS is used in segmentation

##### Contra

* used for very small target regions.
* normalizes read count and log ratios for each base.
* better for reducing effect of GC bias.
* addresses problem of very low coverage and GC bias.

##### cn.MOPS

* Can use multiple samples for each genomics region to create a better estimate
* Uses a non-overlapping sliding window.
* Creates a null-hypothesis to measure distance to observed value.
* Can reduce false discovery rate

##### ExomeCNV

* Specifically design for WES data.
* Can use BAF data
* Counts overlapping reads from exons by using these counts for tumor and normal.
* Can also detect loss of heterzygosity.

##### VarScan2

* Also specifically designed for WES data from tumor-normal.
* Does not use a sliding window.
* Calculates tumor to normal read count ratios of high quality base that fulfill minimum coverage requirements.
* Using Fischers test, consecutive bases do not change significantly are binned together.
* Does not utilize a segmentation algorithm, CBS is advised.

##### Results

* The tools show low consistency.
* only about 25% of the true amplified genes and deleted genes were identified by all tools in the gene-based comparison.
* In the segment-based comparison, the sensitivities and FDRs were all comparably similar to the gene-based comparison.
* There was no consistency among tools for the size and number of detected CNVs as well.
* Due to contra's approach, it excels with clean data but may not be suitable for messy cancer data.
* Mis-detection of poor coverage sequences is a central cause of the poor performance.
* Because of the poor performance across the tools, there is a great need for improvement in preprocessing techniques.
* A point made that may hold relevance to my project is that tumor subclonality has not been addressed by any tool. The granularity offered by our scRNA data could help with this.

##### Other tools

* varsimlab to generate synthetic cnv
* cghmcr to identify cnv genes using refseq gene id
* genomicranges to detect overlapping regions

***

***

# CCGD

#### Project doc location

* `/home/chris/starr_lab/ccgd/refs/CCGD Project.docx`

#### Server Script Conversations

<details>
  <summary>**From Ken re COSMIC changes**</summary>

Possibly--I'll have to see the new file to be sure. We rely on the "gene_name" column in CosmicMutantExport.tsv.gz and the "symbol" column in cancer_gene_census.csv to match our data, and those are the first columns in the files. If they do nothing more than add columns to these files, they may be truncated during loading, and that may not matter so long as the "gene_name" and "symbol" columns are still in frame. I think it's conceivable that they may introduce new 1:many relationships in "gene_name" or "symbol" columns, but that could be handled with a DISTINCT clause in the SQL. The most painful possibility is that they (yet again) change their download process, but that has always been a surmountable problem in the past.



By the way, would you like to renew my sponsored account (abbot195) for another year? There should be a 7/23 email about this in your inbox.



-Ken

</details>

***

### Server site

I was finally able to get through on the access portion of the ccgd project. Sonya played an important role in initiating access, which was ultimately completed by Satish.

* [ccgd-starrlab.oit.umn.edu](ccgd-starrlab.oit.umn.edu)

The working directory for the website seems to be located at:

* `/swadm/var/www/html`



#### Server Access Conversations

<details>
  <summary>**Email thread with Satish to gain access**</summary>

```{r}

Hi Jeff,

Thank you for your response. oialinux@umn.edu was actually the first point of contact I made, and they directed me to HST. I've cc'd the contact who handled the this response.


Satish,

Would we be able to make sense of the discrepancy in administration here?





Thank you,
--
Christopher Tastad

<ctastad@gmail.com>
Mobile: 612-817-7989


On Wed, Jul 17, 2019 at 11:34 AM Jeffrey Sik <sikxx005@umn.edu> wrote:

    Hi Christopher,

    Unfortunately I do not have access to this box and digging further it looks like it's over on OIT's infrastructure. You could try emailing oialinux@umn.edu which is the linux group over there.

    Hope this helps!
    Thanks,


    Jeffrey Sik

    UNIVERSITY OF MINNESOTA
       HST Systems Administrator
       Email: sikxx005@umn.edu Phone: (612) 624-9148


    On Fri, Jul 12, 2019 at 3:35 PM Christopher Tastad <tasta005@umn.edu> wrote:

        Hello,

        I'm forwarding this on looking for some assistance gaining access to a server that is under the management of HST. Would any of you be able to point me in the right direction?




        Thank you,
        --
        Christopher Tastad

        <ctastad@gmail.com>
        Mobile: 612-817-7989


        ---------- Forwarded message ---------
        From: Christopher Tastad <tasta005@umn.edu>
        Date: Tue, Jul 9, 2019 at 4:54 PM
        Subject: Fwd: INC2271719 Update: tasta005@umn.edu - Server Access - CCGD Starr lab - INC2271719 satishs
        To: <kell@umn.edu>
        Cc: Tim Starr <star0044@umn.edu>


        Hello,

        I am reaching out on behalf of the Starr lab regarding an HST hosted server. I will be taking over a required upgrade for this server and need to start by getting access. The server is running RHEL, and I believe the host site is:

        ccgd-starrlab.oit.umn.edu

        Other points of contact I've been given are Jeffrey Sik, Colby Reese, Nathan Huff. If I should contact one of them, please let me know. Otherwise, please advise on how I should proceed.



        Thank you,
        --
        Christopher Tastad

        <ctastad@gmail.com>
        Mobile: 612-817-7989


        ---------- Forwarded message ---------
        From: UMN Service Desk <help@umn.edu>
        Date: Tue, Jul 9, 2019 at 10:20 AM
        Subject: INC2271719 Update: tasta005@umn.edu - Server Access - CCGD Starr lab - INC2271719 satishs
        To: <tasta005@umn.edu>



        Additional comments:
        2019-07-09 10:20:14 CDT - Satish Chowdary Sadineedi (satishs) Additional comments
        Hi Chris ,

        The authorized requestors listed on this host are reese007, kell, nrhuff, sikxx005 and it belongs to HST. . Could you ask one of the authorized requestors to approve this request ?

        Thanks,
        Satish

        You can track the status of your ticket INC2271719 at any time.

        If you would like to provide more information, please reply to this email to update your ticket.

        Ref:MSG25438236


```

</details>


<details>
  <summary>**Email thread with Sonya to gain access**</summary>

Hi, Christopher.

Not a problem. I've asked Satish to grant you access to that server. I've added myself to the watchlist on the ticket.

Thanks,
Sonya
---------------------------------------------------------------------
Sonya Smith Šustáček
Project Manager & Business/Systems Analyst (Infrastructure Services)
Office of Information Technology | it.umn.edu
University of Minnesota | umn.edu
ssustace@umn.edu | 612.301.2128

“We must accept finite disappointment, but never lose infinite hope.”  -- Martin Luther King, Jr.


On Wed, Jul 17, 2019 at 5:58 PM Christopher Tastad <tasta005@umn.edu> wrote:

    Hi Sonya,

    I am reaching out on behalf of the Starr lab regarding a University hosted server. I will be taking over a required upgrade for this server and need to start by getting access.

    With this, I have been having some trouble identifying who owns the administration of the server. Both OIT and HST have indicated that the other is the manager. I still have active threads with them to see if we can sort it out, but I wanted to make another point of contact since this has been a multi week process.

    I've been told reese007, kell, nrhuff, sikxx005 at HST are authorized requestors and have reached out to them with the only response being that OIT is the owner of the space.

    For reference, the server site is below, and my OIT ticket is INC2271719.

    ccgd-starrlab.oit.umn.edu

    Any assistance would be greatly appreciated.



    --
    Christopher Tastad

    <ctastad@gmail.com>
    Mobile: 612-817-7989


    ---------- Forwarded message ---------
    From: Timothy Starr <star0044@umn.edu>
    Date: Wed, Jul 17, 2019 at 4:50 PM
    Subject: Fwd: Strategy to migrate ccgd-starrlab to a RHEL 7 Server
    To: Christopher Tastad <tasta005@umn.edu>




>     Begin forwarded message:
>
>     From: Sonya Sustacek <ssustace@umn.edu>
>     Subject: Re: Strategy to migrate ccgd-starrlab to a RHEL 7 Server
>     Date: April 10, 2019 at 11:26:33 AM CDT
>     To: Ken Abbott <abbot195@umn.edu>
>     Cc: John Trammell <tram0004@umn.edu>, Tim Starr <star0044@umn.edu>, Thomas Kell <kell@umn.edu>
>
>     Hi, Ken.
>
>     It's okay that you cannot attend this meeting. Thanks so much for the information. This is helpful.
>
>     There are a couple of issues that I think it's better for Tim, HST rep, and Linux rep to get together to hammer out and make clear.
>
>     HST can request a new server for Dr. Starr. Then the question is who migrates the website (and updates Phython) to the new server? Who will support it? Does it make sense to move the site to Drupal?
>
>     We (OIT) just want to know where we go from here as things have stalled a bit.
>
>     Again, thanks for you help and good luck with medical school. :-)
>
>     Best,
>     Sonya
>     ---------------------------------------------------------------------
>     Sonya Smith Šustáček
>     Project Manager & Business/Systems Analyst (Infrastructure Services)
>     Office of Information Technology | it.umn.edu
>     University of Minnesota | umn.edu
>     ssustace@umn.edu | 612.301.2128
>
>     “We must accept finite disappointment, but never lose infinite hope.”  -- Martin Luther King, Jr.
>
>
>     On Tue, Apr 9, 2019 at 8:49 PM <abbot195@umn.edu> wrote:
>
>         Hi folks. I built this site for Tim in 2012. I’m a medical student doing 100-hour weeks (until December) so I cannot make it to this meeting, but I hope we can exchange some information via email. I think the best bet is just another RHEL server—with only a few webpages here, I think we don’t need a platform, and it could be a pain to adapt what we have now to work with a platform. I may be able to help with this, but I cannot really speculate much about how much I will be able to accomplish, according to any timetable; I would appreciate a labor estimate (and description of any known hurdles) from someone who has experience completing these RHEL migrations for other University entities.
>
>
>
>         -Ken
>
>
>
>         -----Original Appointment-----
>         From: Google Calendar <calendar-notification@google.com> On Behalf Of Sonya Sustacek
>         Sent: Tuesday, April 9, 2019 4:20 PM
>         To: abbot195@umn.edu; John Trammell; Tim Starr; kell@umn.edu
>         Subject: Strategy to migrate ccgd-starrlab to a RHEL 7 Server
>         When: Friday, April 12, 2019 2:00 PM-2:50 PM (UTC-05:00) Eastern Time (US & Canada).
>         Where: WBOB-523 OIT



</details>

***

### Server Ports

```{r}
Starting Nmap 7.60 ( https://nmap.org ) at 2019-08-14 18:37 CDT
Nmap scan report for ccgd-starrlab.oit.umn.edu (134.84.192.84)
Host is up (0.0089s latency).
Not shown: 997 closed ports
PORT     STATE SERVICE
80/tcp   open  http
2000/tcp open  cisco-sccp
5060/tcp open  sip

Nmap done: 1 IP address (1 host up) scanned in 1.49 seconds

```

### VM Application

#### Standing questions

These are points I need to clarify before submitting the requirements for the new ccgd vm.

* is a oracle license necessary
    + no
* how much drive space is necessary
* determine firewall config ports open
    + see server ports section below
* determine security level for vm
    + the documentation doesn't specify any requirements

#### Application

*Submitted 2019-08-22*


<details>
  <summary>**App response from Tom Kell**</summary>

```{r}

Thanks for the information. I have approved it and you should be hearing from OIT hosting soon.

Tom
---------- Forwarded message ---------
From: <mptflow@umn.edu>
Date: Fri, Aug 23, 2019 at 6:23 AM
Subject: Hosting Service Request for hst-ccgd-prd-web -- linux -- Approve
To: <reese007@umn.edu>, <kell@umn.edu>, <nrhuff@umn.edu>, <sikxx005@umn.edu>




Hello,
The OIT Hosting Services request for hst-ccgd-prd-web has been Approve by kell@umn.edu.

Comments:

No further action is needed.

Thank you,
OIT Hosting Services


--

--
Thomas Kell
Server Operations Manager
Health Sciences Technology
University of Minnesota
kell@umn.edu
612-626-6219

```

</details>

<details>
  <summary>**Question re firewall req**</summary>

```{r}

2019-08-23 11:20:00 CDT - Thomas Bertschinger (thomasb) Additional comments
Hi Christopher,

For the custom firewall, can you confirm whether you need ports 80, 2000, and 5060 open to the world, or just to the UMN?

thanks!
-Thomas

You can track the status of your ticket INC2305839 at any time.

If you would like to provide more information, please reply to this email to update your ticket.
```
</details>

***

***

# Sources

## Tools

* bwa-mem (@Li2013)
* Excavator2 (@DAurizio2016)
* VarScan2 (@Koboldt2012)
* samtools (@Li2009)
* parallel (@Tange2011a)
* DNAcopy [citation needed]
* BEDTools (@Quinlan2010)
* UCSC Table Browser (@Karolchik2004)

## Awesome single cell repo

I found a really comprehensive list of single cell analysis tools at the github below.

* https://github.com/seandavi/awesome-single-cell

## UCSC

UCSC hosts what is effectivley the industry standard for reference genomic content.

#### hg19 ref

* http://hgdownload.cse.ucsc.edu/goldenPath/hg19/bigZips/

#### GR38 ref

* http://hgdownload.cse.ucsc.edu/goldenpath/hg38/bigZips/

#### Genome Browser

* http://genome.ucsc.edu/cgi-bin/hgTables?command=start

## 1kgp

### IGSR

This is a central resource to find data available from the 1kgp.

* https://www.internationalgenome.org/data

###FTP site

This file provides an overview of the structure of the FTP site.

####Top level of the site

At the top level of the site, ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/, there are a number of files and directories.

Files present in ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/:
 - README files: These provide information on a variety of topics related to this FTP site and the data it contains.
 - CHANGELOG file: This file records alterations made to this FTP site.
 - current.tree file: This file lists all directories and files currently present on this FTP site.

Directories present in ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/:
- changelog_details
- data
- data_collections
- historical_data
- phase1
- phase3
- pilot_data
- release
- technical

These directories and described in the next section of this file.

###Directories

####changelog_details

This directory contains a series of files detailing the changes made to the FTP site over time.

####data

The data directory formerly housed data generated during the 1000 Genomes Project. The data that was previously located here has been integrated into data_collections and is present under ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/1000_genomes_project/data. Further information on this move can be found in ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data/README_data_has_moved.md.

####data_collections

The data_collections directory contains directories for various collections of data, typically generated by different projects. Among the data collections is the **1000 Genomes Project** data.

For each collection of data, within the directory for that collection, README and index files provide information on the collection. Under each collection directory, there is a data directory, under which files are organised by population and then sample. Further information can be found in ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/README_data_collections.md.

####historical_data

This directory was created during a rearrangement of the FTP site in September 2015. It houses README and index files that were formerly present at the toplevel of this site, including dedicated index directories. Further information is available in ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/historical_data/README_historical_data.md.

####phase1

This directory contains data that supports the publications associated with phase 1 of the 1000 Genomes Project.

####phase3

This directory contains data that supports the publications associated with phase 3 of the 1000 Genomes Project.

####pilot_data

This directory contains data that supports the publications associated with the pilot phase of the 1000 Genomes Project.

####release

The release directory contains dated directories which contain analysis results sets plus README files explaining how those data sets were produced.

Originally, the date in release subdirectory names was the date on which the given release was made. Thereafter, the release subdirectory dates were based on the date in the name of the corresponding YYYYMMDD.sequence.index file. In future, the date in the directory name will be chosen in a manner appropriate to the data and the nature of the release.

Examples of release subdirectories are:
- ftp://ftp.1000genomes.ebi.ac.uk/release/2008_12/
- ftp://ftp-trace.ncbi.nih.gov/1000genomes/release/2008_12/

In cases where release directories are named based on the date of the YYYYMMDD.sequence.index, the SNP calls, indel calls, etc. in these directories are based on alignments produced from data listed in the  YYYYMMDD.sequence.index file.

For example, the directory
ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20100804/
contains the release versions of SNP and indel calls based on the
ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/historical_data/former_toplevel/sequence_indices/20100804.sequence.index
file.

####technical

The technical directory contains subdirectories for other data sets such as simulations, files for
method development, interm data sets, reference genomes, etc..

An example of data stored under technical is ftp://ftp.1000genomes.ebi.ac.uk/technical/simulations/.

***WARNING: ftp://ftp.1000genomes.ebi.ac.uk/technical/working/***

The working directory under technical contains data that has experimental (non-public release) status
and is suitable for internal project use only. Please use with ***caution***.

###Further information

Should you require further assistance in navigating the FTP site, please contact info@1000genomes.org.

***

## Samtools

### Format Specs

* https://samtools.github.io/hts-specs/SAMv1.pdf

# References
